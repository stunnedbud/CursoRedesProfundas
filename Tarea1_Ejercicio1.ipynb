{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stunnedbud/CursoRedesProfundas/blob/main/Tarea1_Ejercicio1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V83__FrBij1f"
      },
      "source": [
        "# Tarea 1 Ejercicio 1. Red de unidades de umbral lineal\n",
        "\n",
        "En este ejercicio debemos programar y evaluar una red neuronal que aproxime la operación XNOR con funciones de activación escalón unitario. Ésta red se implementó usando NumPy y se entrenará usando gradiente descedente con el algoritmo de retropropagación. \n",
        "\n",
        "Recordemos que la operación XNOR ($\\odot$) se define de la siguiente manera:\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |1|\n",
        "|0 |1 |0|\n",
        "|1 |0 |0|\n",
        "|1 |1 |1|\n",
        "\n",
        "\n",
        "Podemos observar que:\n",
        "\n",
        "$XNOR(x_1, x_2) = OR(NOT(OR(x_1,x_2)AND(x_1,x_2)))\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSlnjW4Oi-FP"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx6SyrPhWBrw"
      },
      "source": [
        "Recordemos que la función de activación escalón unitario se define como:\n",
        "\n",
        "$$\n",
        "\\phi(z) = \\begin{array}{ll}\n",
        "      0 & si & z < 0 \\\\\n",
        "      1 & si & z \\geq 0 \\\\\n",
        "\\end{array} \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYhT3i68jf6x"
      },
      "outputs": [],
      "source": [
        "def escalon_unitario(z):\n",
        "    return 0 if z < 0 else 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8nMdK-RYWMS"
      },
      "source": [
        "Definimos en la siguiente función nuestro modelo de neurona o perceptrón, el cual calcula la suma pesada de sus entradas $x$ y vector de pesos $w$, considerando el sesgo $b$, y evalúa este valor usando la función de activación escalón unitario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wxvZq10jIM3"
      },
      "outputs": [],
      "source": [
        "def perceptron(x, w, b):\n",
        "    v = np.dot(w, x) + b\n",
        "    y = escalon_unitario(v)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iAUmKI5jNuX"
      },
      "source": [
        "\n",
        "### Arquitectura de la red\n",
        "\n",
        "Para computar la función XNOR la red neuronal implementará el siguiente flujo operaciones:\n",
        "<br><br>\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200518230148/XN_p.png\">\n",
        "\n",
        "Como podemos observar en la imagen, la red está compuesta por 4 perceptrones de activación lineal:\n",
        "\n",
        "<ul>\n",
        "<li>El primer nodo OR recibe las dos entradas ($x_1$ y $x_2$) y computa la salida $\\hat{y}_1$ usando sus pesos internos ($w_1$ y $w_2$) y sesgo ($b_{OR}$):\n",
        "$$\n",
        "\\hat{y}_1 = \\phi(w_1x_1+w_2x_2+b_{OR})\n",
        "$$\n",
        "<br>\n",
        "</li>\n",
        "<li>Asimismo el nodo AND recibe las dos entradas ($x_1$ y $x_2$) y computa la salida $\\hat{y}_2$ usando sus pesos internos ($w_1$ y $w_2$) y sesgo ($b_{AND}$):\n",
        "$$\n",
        "\\hat{y}_2 = \\phi(w_1x_1+w_2x_2+b_{AND})\n",
        "$$<br></li>\n",
        "<li>La salida $\\hat{y}_1$ es la entrada del nodo NOT, el cual computa la salida $\\hat{y}_3$ usando su peso interno $w_{NOT}$ y sesgo ($b_{NOT}$):\n",
        "$$\n",
        "\\hat{y}_3 = \\phi(w_{NOT}\\hat{y}_1+b_{NOT})\n",
        "$$<br></li>\n",
        "<li>\n",
        "Las salidas $\\hat{y}_2$, $\\hat{y}_3$ son la entrada del segundo nodo OR, el cual computa la salida final $\\hat{y}$ usando sus pesos internos ($w_{OR1}$ y $w_{OR2}$) y sesgo ($b_{OR}$):\n",
        "$$\n",
        "\\hat{y} = \\phi(w_{OR1}\\hat{y}_3+w_{OR2}\\hat{y}_2+b_{OR})\n",
        "$$<br>\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "Para que funcione la implementación, los valores de los parámetros deben ser los siguientes:\n",
        "$$\n",
        "w_1 = 1, w_2 = 1, w_{NOT} = -1, w_{OR1} = 1, w_{OR2} = 1, b_{AND} = -1.5, b_{OR} = -0.5, b_{NOT} = 0.5\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WxI8FfLXKHv"
      },
      "source": [
        "### Implementación \n",
        "\n",
        "Esta arquitectura y valores de los parámetros fueron codificados usando nuestro modelo de perceptrón previamente definido:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJxvxKeAjn24"
      },
      "outputs": [],
      "source": [
        "# Perceptrón para calcular la función lógica NOT \n",
        "# wNOT = -1, bNOT = 0.5\n",
        "def NOT_logicFunction(x):\n",
        "    wNOT = -1\n",
        "    bNOT = 0.5\n",
        "    return perceptron(x, wNOT, bNOT)\n",
        "  \n",
        "# Perceptrón para calcular la función lógica AND\n",
        "# w1 = 1, w2 = 1, bAND = -1.5\n",
        "def AND_logicFunction(x):\n",
        "    w = np.array([1, 1])\n",
        "    bAND = -1.5\n",
        "    return perceptron(x, w, bAND)\n",
        "  \n",
        "# Perceptrón para calcular la función lógica OR\n",
        "# w1 = wOR1 = 1, \n",
        "# w2 = wOR2 = 1, bOR = -0.5\n",
        "def OR_logicFunction(x):\n",
        "    w = np.array([1, 1])\n",
        "    bOR = -0.5\n",
        "    return perceptron(x, w, bOR)\n",
        "\n",
        "# Perceptrón para calcular la función lógica XNOR\n",
        "# usando las anteriores AND, OR, NOT  \n",
        "def XNOR_logicFunction(x):\n",
        "    y1 = OR_logicFunction(x)\n",
        "    y2 = AND_logicFunction(x)\n",
        "    y3 = NOT_logicFunction(y1)\n",
        "    final_x = np.array([y2, y3])\n",
        "    finalOutput = OR_logicFunction(final_x)\n",
        "    return finalOutput"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Pruebas de funcionamiento\n",
        "\n",
        "Para probar que nuestra red funciona correctamente simplemente debemos probarla en los 4 casos posibles de entradas $x_1$ y $x_2$.\n"
      ],
      "metadata": {
        "id": "aoEmnmxL8hws"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDjlmpAQjR3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6373bcd-b5df-4a4e-e33b-8a380ffb249c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XNOR(0, 1) = 0\n",
            "XNOR(1, 1) = 1\n",
            "XNOR(0, 0) = 1\n",
            "XNOR(1, 0) = 0\n"
          ]
        }
      ],
      "source": [
        "test1 = np.array([0, 1])\n",
        "test2 = np.array([1, 1])\n",
        "test3 = np.array([0, 0])\n",
        "test4 = np.array([1, 0])\n",
        "  \n",
        "print(\"XNOR({}, {}) = {}\".format(0, 1, XNOR_logicFunction(test1)))\n",
        "print(\"XNOR({}, {}) = {}\".format(1, 1, XNOR_logicFunction(test2)))\n",
        "print(\"XNOR({}, {}) = {}\".format(0, 0, XNOR_logicFunction(test3)))\n",
        "print(\"XNOR({}, {}) = {}\".format(1, 0, XNOR_logicFunction(test4)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}