{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stunnedbud/CursoRedesProfundas/blob/main/Tarea2_Ejercicio1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85hL2cbtWiHs"
      },
      "source": [
        "# Tarea 2: 1. Convolución\n",
        "\n",
        "Para este ejercicio adaptaremos la red convolucional implementada en clase para que funcione sobre el conjunto Flowers102 de imagenes a color.\n",
        "\n",
        "[Flowers102](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/) tiene 102 classes teninendo entre 40 y 258 ejemplos por clase. La clasificación de flores es una tarea de grano fino difícil por las siguientes características.\n",
        "\n",
        "* Baja variabilidad inter-clase: las diferencias entre las clases pueden ser sutiles.\n",
        "* Baja variabilidad intra-clase: las diferencias entre los ejemplos de una misma clase pueden ser sutiles.\n",
        "\n",
        "![Flowers102](https://www.researchgate.net/publication/345039740/figure/fig2/AS:1083904253534265@1635434320486/Oxford-102-Flower-dataset-which-contains-102-categories-of-flowers-can-be-used-for.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Conjunto de datos\n",
        "\n",
        "### 1.1 Declarar bibliotecas\n",
        "\n"
      ],
      "metadata": {
        "id": "IiczwVzpFTe_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHzBmFgxB5r_"
      },
      "source": [
        "from os.path import join\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "#import pytorch_lightning as pl\n",
        "from IPython.display import display, HTML\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "#from torchinfo import summary\n",
        "from torchvision.datasets import Flowers102\n",
        "from packaging import version\n",
        "\n",
        "\n",
        "DATA_DIR = '../data'\n",
        "LABELS_URL = 'https://raw.githubusercontent.com/gibranfp/CursoAprendizajeProfundo/2023-1/data/flowers-102/labels.csv'\n",
        "# for Kaggle, fix Flowers102 labels for torchvision < 0.13.0\n",
        "FIX_LABELS = version.parse(torchvision.__version__) < version.parse('0.13.0')\n",
        "target_transform = T.Lambda(lambda y: y - 1) if FIX_LABELS else nn.Identity()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W02429ROng15"
      },
      "source": [
        "## 1.2 Carga del conjunto de datos\n",
        "Vamos a cargar el conjunto de imagenes de flores con la biblioteca que ofrece 'torchvision':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8Sxl3zRoe7W"
      },
      "source": [
        "#digits = load_digits() \n",
        "#zeros_ones = digits.target < 2\n",
        "#data = digits.images[zeros_ones]\n",
        "#labels = digits.target[zeros_ones]\n",
        "\n",
        "trn_ds = Flowers102(DATA_DIR, 'train', target_transform=target_transform, download=True)\n",
        "val_ds = Flowers102(DATA_DIR, 'val', target_transform=target_transform)\n",
        "LABELS = pd.read_csv(LABELS_URL).labels\n",
        "\n",
        "f'There are {len(trn_ds)} training and {len(val_ds)} validation examples'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sids-j5x2FS"
      },
      "source": [
        "Podemos visualizar una imagen del conjunto de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba91f2d6"
      },
      "outputs": [],
      "source": [
        "x, y = trn_ds[0]\n",
        "npx = np.array(x)\n",
        "print(f'Shape of single image: {npx.shape}')\n",
        "print(f'Image PIL: {x}')\n",
        "print(f'Mean of pixel values: {np.array(x).mean()}')\n",
        "print(f'Label numeric: {y}, string: {LABELS[y]}')\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebd1c21d"
      },
      "source": [
        "### 1.3 Tuberia de datos\n",
        "\n",
        "Un [tuberia de datos](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) es la serie de pasos para leer y preprocesar los datos dejandolos listo para ser ingresar al modelo. Una tuberia tiene dos clases principales:\n",
        "\n",
        "* [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset): carga un ejemplo y aplica una transformación.\n",
        "* [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader): agrupa los ejemplos y los barajea si es necesario (entrenamiento).\n",
        "\n",
        "![Data Pipeline](https://raw.githubusercontent.com/bereml/riiaa-22-tl/master/figs/data_pipeline.svg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c73b8fd8"
      },
      "outputs": [],
      "source": [
        "PRETRAIN_NORM = {\n",
        "   'None': ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "   'ImageNet1k': ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "   'ImageNet21k': ((0., 0., 0.), (1., 1., 1.))\n",
        "}\n",
        "\n",
        "\n",
        "def build_dl(subset, pretraining):\n",
        "    \"\"\"Builds a dataloader for the `subset` with the correct stats.\"\"\"\n",
        "    # get mean and standard deviation\n",
        "    mean, std = PRETRAIN_NORM.get(pretraining, 'None')\n",
        "\n",
        "    # assamble preprossing pipeline\n",
        "    transform = T.Compose([\n",
        "        # resize to small image\n",
        "        T.Resize(256),\n",
        "        # crop to te Center\n",
        "        T.CenterCrop(224),\n",
        "        # PIL to Tensor and [0-255] to [0, 1]\n",
        "        T.ToTensor(),\n",
        "        # normalize with stats\n",
        "        T.Normalize(mean, std),\n",
        "    ])\n",
        "\n",
        "    # create the dataset\n",
        "    dataset = Flowers102(DATA_DIR, subset, transform, target_transform)\n",
        "\n",
        "    # create the dataloader to pack batches\n",
        "    dataloader = DataLoader(\n",
        "        dataset, \n",
        "        batch_size=32, \n",
        "        num_workers=4, \n",
        "        shuffle=subset=='train'\n",
        "    )\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "# creates a dataloader\n",
        "val_dl = build_dl('val', 'None')\n",
        "\n",
        "# loads the first batch\n",
        "batch = next(iter(val_dl))\n",
        "x, y = batch\n",
        "\n",
        "# inspects batch\n",
        "print(f'x.shape: {x.shape} x.mean: {x.mean()}')\n",
        "print(f'y.shape: {y.shape} y[0]: {y[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6QRdNNaCmc8"
      },
      "source": [
        "## 2. Convolución y correlación cruzada\n",
        "\n",
        "#### 2.1 En 1 canal\n",
        "La operación de convolución entre una imagen en escala de grises $I$ y un filtro $W$ está definida por\n",
        "\n",
        "$$\n",
        "A_{i,j} = (\\mathbf{I} * \\mathbf{W})_{i,j} = \\sum_m \\sum_n I_{m, n} W_{i - m, j - n}\n",
        "$$\n",
        "\n",
        "La convolución es commutativa, por lo tanto \n",
        "\n",
        "$$\n",
        "A_{i,j} = (\\mathbf{W} * \\mathbf{I})_{i,j} = \\sum_m \\sum_n I_{i - m, j - n} W_{m,n}\n",
        "$$\n",
        "\n",
        "En lugar de la convolución, frecuentemente se ocupa la operación de correlación cruzada para llevar a cabo las capas convolucionales. Esta operación es similar a la convolución pero sin voltear el filtro (por lo que pierde la propiedad de conmutatividad) y está dada por\n",
        "\n",
        "$$\n",
        "A_{i,j} = (\\mathbf{W} * \\mathbf{I})_{i,j} = \\sum_m \\sum_n I_{i + m, j + n} W_{m,n} \n",
        "$$\n",
        "\n",
        "\n",
        "#### 2.2 En 3 canales\n",
        "\n",
        "Para imagenes a color tenemos varias opciones de cómo aplicar filtros convolucionales. Podemos aplicar un filtro independiente por cada capa, resultando en 3 salidas o una salida con profundidad 3. O bien lo que haremos aquí es aplicar un filtro de tamaño 3 x 3 x 3 a las 3 capas al mismo tiempo para reducir a un sólo valor. \n",
        "\n",
        "La implementación en NumPy sería casi identica a la convolución en 2d, gracias a la sobrecarga del operador '*'. Solamente que ahora el filtro de entrada W y la imagen I deben ser de 3 dimensiones.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK60sSz6pMgk"
      },
      "source": [
        "def conv3d(I, W, b, stride = 1):\n",
        "  h_s = int(np.floor((I.shape[0] - W.shape[0]) / stride)) + 1\n",
        "  w_s = int(np.floor((I.shape[1] - W.shape[1]) / stride)) + 1\n",
        "  a = np.zeros((h_s, w_s))\n",
        "  for i in range(h_s):\n",
        "    for j in range(w_s):\n",
        "      I_m = I[i * stride:i * stride + W.shape[0], j * stride:j * stride + W.shape[1], :] # unico cambio necesario \n",
        "      a[i, j] = (I_m * W).sum() + b\n",
        "                  \n",
        "  return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY9UBGoqhCDR"
      },
      "source": [
        "## Filtro\n",
        "\n",
        "Podemos definir cualquier filtro de 3 dimensiones. Por ejemplo uno de tamaño $3 \\times 3 \\times 3$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26O7xja9hDYM"
      },
      "source": [
        "# Activando la diagonal del filtro en 3d\n",
        "filter1 = np.zeros((3,3,3))\n",
        "np.fill_diagonal(filter1, np.array([1, 1, 1]))\n",
        "print(filter1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activando las esquinas de cada capa\n",
        "filter2 = np.array([[[1, 0, 1],[0, 0, 0],[1, 0, 1]],[[1, 0, 1],[0, 0, 0],[1, 0, 1]],[[1, 0, 1],[0, 0, 0],[1, 0, 1]]])\n",
        "# Convolucionando todo en un parche de 3x3x3\n",
        "filter3 = np.ones((3,3,3))\n",
        "# Solo la esquina superior izq\n",
        "filter4 = np.zeros((3,3,3))\n",
        "filter4[0,0,0] = 1\n",
        "\n",
        "filters = np.array([filter1, filter2, filter3, filter4])"
      ],
      "metadata": {
        "id": "XAsK3LllZ6gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYS40PWxH7Su"
      },
      "source": [
        "Aplicando las operaciones de correlación cruzada y convolución de una imagen del dígito $0$ con dicho filtro y al revés obtenemos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoyByBCuhK2Y"
      },
      "source": [
        "x, y = trn_ds[0]\n",
        "npx = np.asarray(x)\n",
        "a1 = conv3d(npx, filter1, 3)\n",
        "a2 = conv3d(npx, filter2, 5)\n",
        "a3 = conv3d(npx, filter3, 1)\n",
        "a4 = conv3d(npx, filter4, 2)\n",
        "fig, axs = plt.subplots(1, 5, figsize=(10, 5))\n",
        "axs[0].imshow(x, cmap = 'gray') \n",
        "axs[1].imshow(a1, cmap = 'gray') \n",
        "axs[2].imshow(a2, cmap = 'gray') \n",
        "axs[3].imshow(a3, cmap = 'gray')\n",
        "axs[4].imshow(a4, cmap = 'gray')  \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9BE4QqWIX3W"
      },
      "source": [
        "La imagen de la izquierda es la original, y las otras 4 son el mapa de activaciones en escala de grises, resultantes de aplicar cada filtro implementado. Podemos ver que son apenas ligeramente diferentes entre sí.\n",
        "\n",
        "Podemos probar lo mismo con otra imagen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlX6n1YgA6p0"
      },
      "source": [
        "x, y = trn_ds[1]\n",
        "npx = np.asarray(x)\n",
        "a1 = conv3d(npx, filter1, 3)\n",
        "a2 = conv3d(npx, filter2, 5)\n",
        "a3 = conv3d(npx, filter3, 1)\n",
        "a4 = conv3d(npx, filter4, 2)\n",
        "fig, axs = plt.subplots(1, 5, figsize=(10, 5))\n",
        "axs[0].imshow(x, cmap = 'gray') \n",
        "axs[1].imshow(a1, cmap = 'gray') \n",
        "axs[2].imshow(a2, cmap = 'gray') \n",
        "axs[3].imshow(a3, cmap = 'gray')\n",
        "axs[4].imshow(a4, cmap = 'gray')  \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rWqf1OfW_RW"
      },
      "source": [
        "Para aplicar la operación de convolución con cada uno de estos filtros definimos la siguiente función:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHDYREZ0im1w"
      },
      "source": [
        "def multi_conv3d(I, W, b, stride = 1):\n",
        "  k = len(W)\n",
        "  activations = []\n",
        "  for i in range(k):\n",
        "    activations.append(conv3d(I, W[i], b[i], stride = 1)) \n",
        "                       \n",
        "  return np.array(activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syOtzu1ySCLB"
      },
      "source": [
        "Evaluamos esta función con cuatro distintos filtros para nuestras dos imágenes, obteniendo cuatro mapas de activaciones por cada imagen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y--qB6r0K-HT"
      },
      "source": [
        "x0, _ = trn_ds[0]\n",
        "x1, _ = trn_ds[1]\n",
        "npx0 = np.asarray(x0)\n",
        "npx1 = np.asarray(x1)\n",
        "b = [1,2,3,4] \n",
        "activations_0 = np.tanh(multi_conv3d(npx0, filters, b))\n",
        "activations_1 = np.tanh(multi_conv3d(npx1, filters, b))\n",
        "\n",
        "for i in range(4):\n",
        "  plt.subplot(3,4,i + 1)\n",
        "  plt.imshow(filters[i], cmap = 'gray')\n",
        "  plt.subplot(3,4,i + 5)\n",
        "  plt.imshow(activations_0[i], cmap = 'gray')\n",
        "  plt.subplot(3,4,i + 9)\n",
        "  plt.imshow(activations_1[i], cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqeGeXo_XiiE"
      },
      "source": [
        "Ahora definimos funciones para realizar un submuestreo máximo a un conjunto de mapas de características:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_IcRjX13VGV"
      },
      "source": [
        "def submuestreo_maximo(activations, block = (2,2)):\n",
        "  H, W = activations.shape\n",
        "  H_s = H // block[0]\n",
        "  W_s = W // block[1]\n",
        "\n",
        "  sub_a = np.zeros((H_s,W_s))\n",
        "  max_x = activations.reshape((H * W_s, block[1])).max(axis = 1)\n",
        "  sub_a = max_x.T.reshape((W_s, block[0], H_s)).max(axis = 1)\n",
        "  \n",
        "  return sub_a\n",
        "\n",
        "def multi_submuestreo_maximo(activations, block = (2,2)):\n",
        "  k = activations.shape[0]\n",
        "  sub_a = []\n",
        "  for i in range(k):\n",
        "    sub_a.append(submuestreo_maximo(activations[i]))\n",
        "                       \n",
        "  return np.array(sub_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8BGPZGKS0FS"
      },
      "source": [
        "Aplicamos el submuestreo a nuestros mapas de activaciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCqTIsIqqdox"
      },
      "source": [
        "sub_a0 = multi_submuestreo_maximo(activations_0)\n",
        "sub_a1 = multi_submuestreo_maximo(activations_1)\n",
        "for i in range(4):  \n",
        "\n",
        "  plt.subplot(2,4,i + 1)\n",
        "  plt.imshow(sub_a0[i], vmin = 0, cmap = 'gray')\n",
        "  plt.subplot(2,4,i + 5)\n",
        "  plt.imshow(sub_a1[i], vmin = 0, cmap = 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}